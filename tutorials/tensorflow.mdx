---
title: TensorFlow
description: With CUDO Compute you can deploy TensorFlow docker containers to the latest NVIDIA Ampere Architecture GPUs.
---

TensorFlow is an open source framework for machine learning. With CUDO Compute you can deploy TensorFlow docker containers to the latest NVIDIA Ampere Architecture GPUs.
Prebuilt images with NVIDIA drivers and docker and ready to deploy in the marketplace.

Common uses for TensorFlow:

- Deep Neural Networks (DNN)
- Convolutional Neural Networks (CNN)
- Conversational AI
- Recurrent Neural Networks (RNN)
- Reinforcement Learning
- Natural Language Processing (NLP)

## Quick start guide

1. Prerequisites
2. TensorFlow with Docker
3. TensorFlow Serving with Docker

## Prerequisites

- Create a project and [add an SSH key](/tutorials/how-to-generate-ssh-keys)
- Optionally download [CLI tool](/cli-tool)
- Choose a virtual machine with an NVIDIA GPU and Configure
- Use the **Ubuntu 22.04 + NVIDIA drivers + Docker** image (in CLI tool type `-image ubuntu-nvidia-docker`)

## Running TensorFlow on CUDO Compute with Docker

SSH into your virtual machine and run the following commands

```shell
docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu
```

Or for the NVIDIA optimized TensorFlow container

```shell
docker run --gpus all -it --rm nvcr.io/nvidia/tensorflow:22.08-tf2-py3
```

NGC tags can be found [here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow/tags)

At the prompt

```shell
python
>>> import tensorflow as tf
>>> tf.config.list_physical_devices("GPU")
```

## TensorFlow serving on CUDO Compute

```shell
docker pull tensorflow/serving:latest-gpu
```

Try an example model:

```shell
mkdir -p /tmp/tfserving
cd /tmp/tfserving
git clone https://github.com/tensorflow/serving
```

```shell
docker run --gpus all -p 8501:8501 \
--mount type=bind,\
source=/tmp/tfserving/serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_gpu,\
target=/models/half_plus_two \
  -e MODEL_NAME=half_plus_two -t tensorflow/serving:latest-gpu &
```

```shell
curl -d '{"instances": [1.0, 2.0, 5.0]}' \
  -X POST http://PUBLIC_IP_ADDRESS:8501/v1/models/half_plus_two:predict
```

[Get more information in the official docs](https://www.tensorflow.org/tfx/serving/docker#running_a_gpu_serving_image)

